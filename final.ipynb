{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in ./venv/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.9/site-packages (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./venv/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./venv/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.9/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.9/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./venv/lib/python3.9/site-packages (from spacy) (1.10.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./venv/lib/python3.9/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./venv/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./venv/lib/python3.9/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./venv/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.9/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./venv/lib/python3.9/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./venv/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.9/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.9/site-packages (from jinja2->spacy) (2.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/hackerman/ml-cybersec/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('enron_spam_data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message ID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam/Ham</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>christmas tree farm pictures</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>vastar resources , inc .</td>\n",
       "      <td>gary , production from the high island larger ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>calpine daily gas nomination</td>\n",
       "      <td>- calpine daily gas nomination 1 . doc</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>re : issue</td>\n",
       "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>meter 7268 nov allocation</td>\n",
       "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Message ID                       Subject  \\\n",
       "0           0  christmas tree farm pictures   \n",
       "1           1      vastar resources , inc .   \n",
       "2           2  calpine daily gas nomination   \n",
       "3           3                    re : issue   \n",
       "4           4     meter 7268 nov allocation   \n",
       "\n",
       "                                             Message Spam/Ham        Date  \n",
       "0                                                NaN      ham  1999-12-10  \n",
       "1  gary , production from the high island larger ...      ham  1999-12-13  \n",
       "2             - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n",
       "3  fyi - see note below - already done .\\nstella\\...      ham  1999-12-14  \n",
       "4  fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham  1999-12-14  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam    50.928343\n",
       "ham     49.071657\n",
       "Name: Spam/Ham, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Spam/Ham'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Spam\"] = pd.get_dummies(data[\"Spam/Ham\"])[\"spam\"]\n",
    "data = data.drop(columns=[\"Message ID\", \"Date\", \"Spam/Ham\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33711</th>\n",
       "      <td>= ? iso - 8859 - 1 ? q ? good _ news _ c = eda...</td>\n",
       "      <td>hello , welcome to gigapharm onlinne shop .\\np...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33712</th>\n",
       "      <td>all prescript medicines are on special . to be...</td>\n",
       "      <td>i got it earlier than expected and it was wrap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33713</th>\n",
       "      <td>the next generation online pharmacy .</td>\n",
       "      <td>are you ready to rock on ? let the man in you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33714</th>\n",
       "      <td>bloow in 5 - 10 times the time</td>\n",
       "      <td>learn how to last 5 - 10 times longer in\\nbed ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33715</th>\n",
       "      <td>dear sir , i am interested in it</td>\n",
       "      <td>hi : )\\ndo you need some softwares ? i can giv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Subject  \\\n",
       "33711  = ? iso - 8859 - 1 ? q ? good _ news _ c = eda...   \n",
       "33712  all prescript medicines are on special . to be...   \n",
       "33713              the next generation online pharmacy .   \n",
       "33714                     bloow in 5 - 10 times the time   \n",
       "33715                   dear sir , i am interested in it   \n",
       "\n",
       "                                                 Message  Spam  \n",
       "33711  hello , welcome to gigapharm onlinne shop .\\np...     1  \n",
       "33712  i got it earlier than expected and it was wrap...     1  \n",
       "33713  are you ready to rock on ? let the man in you ...     1  \n",
       "33714  learn how to last 5 - 10 times longer in\\nbed ...     1  \n",
       "33715  hi : )\\ndo you need some softwares ? i can giv...     1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"cleaned_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>33716.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.509283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Spam\n",
       "count  33716.000000\n",
       "mean       0.509283\n",
       "std        0.499921\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        1.000000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33716 entries, 0 to 33715\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Subject  33427 non-null  object\n",
      " 1   Message  33345 non-null  object\n",
      " 2   Spam     33716 non-null  uint8 \n",
      "dtypes: object(2), uint8(1)\n",
      "memory usage: 559.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = data[[\"Subject\", \"Spam\"]].dropna()\n",
    "messages = data[[\"Message\", \"Spam\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = data[[\"Subject\", \"Spam\"]].dropna()\n",
    "messages = data[[\"Message\", \"Spam\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 33427 entries, 0 to 33715\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Subject  33427 non-null  object\n",
      " 1   Spam     33427 non-null  uint8 \n",
      "dtypes: object(1), uint8(1)\n",
      "memory usage: 554.9+ KB\n"
     ]
    }
   ],
   "source": [
    "subjects.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 33345 entries, 1 to 33715\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Message  33345 non-null  object\n",
      " 1   Spam     33345 non-null  uint8 \n",
      "dtypes: object(1), uint8(1)\n",
      "memory usage: 553.6+ KB\n"
     ]
    }
   ],
   "source": [
    "messages.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = data.sample(n=32000, random_state=42).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>discount meds right from home\\nvalium , xanax ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3504</td>\n",
       "      <td>neon</td>\n",
       "      <td>- bammel neon groups - fall 2001 . doc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30823</td>\n",
       "      <td>download great shots of entertainment people</td>\n",
       "      <td>yo buddy ,\\ncaught in action - obscene footage...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22501</td>\n",
       "      <td>hi again</td>\n",
       "      <td>don ' t protect part reach . noon sleep tail m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29593</td>\n",
       "      <td>congratulations ! ? ? ?</td>\n",
       "      <td>from the desk of the vice president\\nluckyday ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>661</td>\n",
       "      <td>brandywine meter # : 981225 ; march , 2000 act...</td>\n",
       "      <td>there was no flow at meter 981225 for the mont...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>16082</td>\n",
       "      <td>pharmacy - no prescription required</td>\n",
       "      <td>online pharmacy - visit our online store and s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>7057</td>\n",
       "      <td>university of texas conference on energy finan...</td>\n",
       "      <td>jeff ,\\nour friends at the university of texas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>10181</td>\n",
       "      <td>are you listed in major search engines ?</td>\n",
       "      <td>submitting your website in search engines may ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>20049</td>\n",
       "      <td>next step</td>\n",
       "      <td>bun headlands hyper snorkel\\nprocrastinating b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            Subject  \\\n",
       "0       3820                                                NaN   \n",
       "1       3504                                               neon   \n",
       "2      30823       download great shots of entertainment people   \n",
       "3      22501                                           hi again   \n",
       "4      29593                            congratulations ! ? ? ?   \n",
       "...      ...                                                ...   \n",
       "31995    661  brandywine meter # : 981225 ; march , 2000 act...   \n",
       "31996  16082                pharmacy - no prescription required   \n",
       "31997   7057  university of texas conference on energy finan...   \n",
       "31998  10181           are you listed in major search engines ?   \n",
       "31999  20049                                          next step   \n",
       "\n",
       "                                                 Message  Spam  \n",
       "0      discount meds right from home\\nvalium , xanax ...     1  \n",
       "1                 - bammel neon groups - fall 2001 . doc     0  \n",
       "2      yo buddy ,\\ncaught in action - obscene footage...     1  \n",
       "3      don ' t protect part reach . noon sleep tail m...     1  \n",
       "4      from the desk of the vice president\\nluckyday ...     1  \n",
       "...                                                  ...   ...  \n",
       "31995  there was no flow at meter 981225 for the mont...     0  \n",
       "31996  online pharmacy - visit our online store and s...     1  \n",
       "31997  jeff ,\\nour friends at the university of texas...     0  \n",
       "31998  submitting your website in search engines may ...     1  \n",
       "31999  bun headlands hyper snorkel\\nprocrastinating b...     1  \n",
       "\n",
       "[32000 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample.to_csv(\"cleaned_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_sample = data_sample[[\"Subject\", \"Spam\"]].dropna()\n",
    "messages_sample = data_sample[[\"Message\", \"Spam\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_sample.to_csv(\"subjects_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_sample.to_csv(\"messages_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subjects' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m subjects\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39msubjects.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subjects' is not defined"
     ]
    }
   ],
   "source": [
    "subjects.to_csv(\"subjects.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.to_csv(\"messages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    16852\n",
       "0    16493\n",
       "Name: Spam, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[\"Spam\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 42.8 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in ./venv/lib/python3.9/site-packages (from en-core-web-md==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (58.0.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/hackerman/ml-cybersec/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(messages_sample, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "db = DocBin()\n",
    "for index, row in train_df.iterrows():\n",
    "    text = nlp.make_doc(row[\"Message\"])\n",
    "    text.cats = {\"SPAM\": row[\"Spam\"], \"NOT SPAM\": 1 - row[\"Spam\"]}\n",
    "    db.add(text)\n",
    "db.to_disk(\"train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "db = DocBin()\n",
    "for index, row in test_df.iterrows():\n",
    "    text = nlp.make_doc(row[\"Message\"])\n",
    "    text.cats = {\"SPAM\": row[\"Spam\"], \"NOT SPAM\": 1 - row[\"Spam\"]}\n",
    "    db.add(text)\n",
    "db.to_disk(\"test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for index, row in train_df.iterrows():\n",
    "    text = row[\"Message\"]\n",
    "    label = {\"cats\": {\"SPAM\": row[\"Spam\"], \"NOT SPAM\": 1 - row[\"Spam\"]}}\n",
    "    example = Example.from_dict(nlp.make_doc(text), {\"cats\": label})\n",
    "    train_data.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.pipeline.textcat import single_label_cnn_config\n",
    "nlp.add_pipe(\"textcat\", config={\"model\": \n",
    "    {\"@architectures\": \"spacy.TextCatCNN.v2\",\n",
    "    \"exclusive_classes\": True,\n",
    "    \"tok2vec\": \n",
    "        {\"@architectures\": \"spacy.HashEmbedCNN.v2\",\n",
    "        \"width\": 128,\n",
    "        \"window_size\": 1,\n",
    "        \"maxout_pieces\": 3,\n",
    "        \"depth\": 4,\n",
    "        \"embed_size\": 2000,\n",
    "        \"pretrained_vectors\": None,\n",
    "        \"subword_features\": True}}}, last=True)\n",
    "textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "textcat.add_label(\"SPAM\")\n",
    "textcat.add_label(\"NOT SPAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Parameter 'E' for model 'hashembed' has not been allocated yet.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39mfor\u001b[39;00m text, annot \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m     11\u001b[0m         examples\u001b[39m.\u001b[39mappend(Example\u001b[39m.\u001b[39mfrom_dict(nlp\u001b[39m.\u001b[39mmake_doc(text), annot))\n\u001b[0;32m---> 12\u001b[0m     nlp\u001b[39m.\u001b[39;49mupdate(examples, drop\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, losses\u001b[39m=\u001b[39;49mlosses)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIteration \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: Losses - \u001b[39m\u001b[39m{\u001b[39;00mlosses\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/spacy/language.py:1155\u001b[0m, in \u001b[0;36mLanguage.update\u001b[0;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[39mfor\u001b[39;00m name, proc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline:\n\u001b[1;32m   1153\u001b[0m     \u001b[39m# ignore statements are used here because mypy ignores hasattr\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mupdate\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1155\u001b[0m         proc\u001b[39m.\u001b[39;49mupdate(examples, sgd\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, losses\u001b[39m=\u001b[39;49mlosses, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg[name])  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m     \u001b[39mif\u001b[39;00m sgd \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1157\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1158\u001b[0m             name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude\n\u001b[1;32m   1159\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(proc, ty\u001b[39m.\u001b[39mTrainableComponent)\n\u001b[1;32m   1160\u001b[0m             \u001b[39mand\u001b[39;00m proc\u001b[39m.\u001b[39mis_trainable\n\u001b[1;32m   1161\u001b[0m             \u001b[39mand\u001b[39;00m proc\u001b[39m.\u001b[39mmodel \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1162\u001b[0m         ):\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/spacy/pipeline/textcat.py:247\u001b[0m, in \u001b[0;36mTextCategorizer.update\u001b[0;34m(self, examples, drop, sgd, losses)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m losses\n\u001b[1;32m    246\u001b[0m set_dropout_rate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, drop)\n\u001b[0;32m--> 247\u001b[0m scores, bp_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mbegin_update([eg\u001b[39m.\u001b[39;49mpredicted \u001b[39mfor\u001b[39;49;00m eg \u001b[39min\u001b[39;49;00m examples])\n\u001b[1;32m    248\u001b[0m loss, d_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_loss(examples, scores)\n\u001b[1;32m    249\u001b[0m bp_scores(d_scores)\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/model.py:309\u001b[0m, in \u001b[0;36mModel.begin_update\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbegin_update\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable[[OutT], InT]]:\n\u001b[1;32m    303\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run the model over a batch of data, returning the output and a\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m    callback to complete the backward pass. A tuple (Y, finish_update),\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39m    where Y is a batch of output data, and finish_update is a callback that\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[39m    takes the gradient with respect to the output and an optimizer function,\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39m    and returns the gradient with respect to the input.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 291 (2 times), forward at line 55 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/layers/with_array.py:32\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     29\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m     30\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Ragged):\n\u001b[0;32m---> 32\u001b[0m         \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _ragged_forward(model, Xseq, is_train))\n\u001b[1;32m     33\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Padded):\n\u001b[1;32m     34\u001b[0m         \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/layers/with_array.py:87\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[0;34m(model, Xr, is_train)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ragged_forward\u001b[39m(\n\u001b[1;32m     84\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m     85\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[1;32m     86\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 87\u001b[0m     Y, get_dX \u001b[39m=\u001b[39m layer(Xr\u001b[39m.\u001b[39;49mdataXd, is_train)\n\u001b[1;32m     89\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYr: Ragged) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Ragged:\n\u001b[1;32m     90\u001b[0m         \u001b[39mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[39m.\u001b[39mdataXd), dYr\u001b[39m.\u001b[39mlengths)\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/layers/concatenate.py:44\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m---> 44\u001b[0m     Ys, callbacks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[layer(X, is_train\u001b[39m=\u001b[39mis_train) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers])\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Ys[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[1;32m     46\u001b[0m         data_l, backprop \u001b[39m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/layers/concatenate.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m---> 44\u001b[0m     Ys, callbacks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[layer(X, is_train\u001b[39m=\u001b[39;49mis_train) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers])\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Ys[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[1;32m     46\u001b[0m         data_l, backprop \u001b[39m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/layers/hashembed.py:63\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, ids, is_train)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     61\u001b[0m     model: Model[Ints1d, OutT], ids: Ints1d, is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m     62\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m---> 63\u001b[0m     vectors \u001b[39m=\u001b[39m cast(Floats2d, model\u001b[39m.\u001b[39;49mget_param(\u001b[39m\"\u001b[39;49m\u001b[39mE\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     64\u001b[0m     nV \u001b[39m=\u001b[39m vectors\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     65\u001b[0m     nO \u001b[39m=\u001b[39m vectors\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/ml-cybersec/venv/lib/python3.9/site-packages/thinc/model.py:216\u001b[0m, in \u001b[0;36mModel.get_param\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown param: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for model \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params\u001b[39m.\u001b[39mhas_param(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid, name):\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    217\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for model \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has not been allocated yet.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params\u001b[39m.\u001b[39mget_param(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid, name)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Parameter 'E' for model 'hashembed' has not been allocated yet.\""
     ]
    }
   ],
   "source": [
    "n_iter = 10\n",
    "optimizer = nlp.create_optimizer()\n",
    "\n",
    "print(\"Training the model...\")\n",
    "for i in range(n_iter):\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        examples = []\n",
    "        for text, annot in batch:\n",
    "            examples.append(Example.from_dict(nlp.make_doc(text), annot))\n",
    "        nlp.update(examples, drop=0.2, losses=losses)\n",
    "    print(f\"Iteration {i}: Losses - {losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "textcat_config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train textcat_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config ./base_config.cfg ./textcat_config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-03-14 21:35:29,636] [INFO] Set up nlp object from config\n",
      "[2023-03-14 21:35:29,644] [INFO] Pipeline: ['tok2vec', 'textcat']\n",
      "[2023-03-14 21:35:29,647] [INFO] Created vocabulary\n",
      "[2023-03-14 21:35:30,822] [INFO] Added vectors: en_core_web_md\n",
      "[2023-03-14 21:35:30,926] [INFO] Finished initializing nlp object\n",
      "[2023-03-14 21:35:46,402] [INFO] Initialized pipeline components: ['tok2vec', 'textcat']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ------------  ----------  ------\n",
      "  0       0          0.00          0.25       35.76    0.36\n",
      "  0     200         29.72         61.96       44.98    0.45\n",
      "  0     400         72.65         68.88       69.29    0.69\n",
      "  0     600         52.00         50.07       70.50    0.70\n",
      "  0     800        175.74         50.26       76.92    0.77\n",
      "  0    1000        154.03         51.62       76.36    0.76\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./textcat_config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./test.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "text_cls = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"bow\"})\n",
    "\n",
    "# add pipeline in model we can add other steps in pipeline also but for now i am not adding tokenization, lemmetization, stop word removation etc. steps\n",
    "nlp.add_pipe(text_cls)\n",
    "\n",
    "# add your customer label in pipeline\n",
    "text_cls.add_label('ham')\n",
    "text_cls.add_label('spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['text'], data['target'], test_size=0.3, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and test data for the spacy model\n",
    "train_lables = [{'cats': {'ham': label == 'ham',\n",
    "                          'spam': label == 'spam'}}  for label in y_train]\n",
    "test_lables = [{'cats': {'ham': label == 'ham',\n",
    "                      'spam': label == 'spam'}}  for label in y_test]\n",
    "\n",
    "# Spacy model data\n",
    "train_data = list(zip(x_train, train_lables))\n",
    "test_data = list(zip(x_test, test_lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, optimizer, batch_size, epochs=10):\n",
    "    losses = {}\n",
    "    random.seed(1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(train_data)\n",
    "\n",
    "        batches = minibatch(train_data, size=batch_size)\n",
    "        for batch in batches:\n",
    "            # Split batch into texts and labels\n",
    "            texts, labels = zip(*batch)\n",
    "\n",
    "            # Update model with texts and labels\n",
    "            model.update(texts, labels, sgd=optimizer, losses=losses)\n",
    "        print(\"Loss: {}\".format(losses['textcat']))\n",
    "\n",
    "    return losses['textcat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.begin_training()\n",
    "batch_size = 5\n",
    "epochs = 20\n",
    "\n",
    "# Training the model\n",
    "train_model(nlp, train_data, optimizer, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, texts):\n",
    "    # Use the model's tokenizer to tokenize each input text\n",
    "    docs = [model.tokenizer(text) for text in texts]\n",
    "\n",
    "    # Use textcat to get the scores for each doc\n",
    "    text_cls = model.get_pipe('textcat')\n",
    "    scores, _ = text_cls.predict(docs)\n",
    "\n",
    "    # From the scores, find the label with the highest score/probability\n",
    "    predicted_labels = scores.argmax(axis=1)\n",
    "    predicted_class = [text_cls.labels[label] for label in predicted_labels]\n",
    "\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = get_predictions(nlp, x_train)\n",
    "test_predictions = get_predictions(nlp, x_test)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_accuracy))\n",
    "print(\"Test accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SPAM': 0.985451877117157, 'NOT SPAM': 0.014548069797456264}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"output_md_20000_sample16000_4/model-best\")\n",
    "test_text=\"SALE!\"\n",
    "doc=nlp(test_text)\n",
    "doc.cats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hackerman/ml-cybersec/venv/lib/python3.9/site-packages/spacy/util.py:887: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"output_md_20000_sample16000_4/model-last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the internet's online pharmacy viagra - xenica...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dear consumers, increase your business sales! ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1) fight the risk of cancer! http://www.adclic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we are offering you quality marketing lists wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;html&gt; &lt;body&gt; &lt;font face=\"ms sans serif\"&gt; &lt;fon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>&gt;&gt;&gt;&gt;&gt; \"g\" == geege schuman &lt;geege@barrera.org&gt;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>&gt; i'm not sure what you mean by \"let's you and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>&gt;&gt;&gt;robert elz said: &gt; date: wed, 28 aug 2002 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>chuck murcko wrote: &gt; heh, ten years ago sayin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>how about this: a bored forker said: \"yawn\" i ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Message  Spam\n",
       "0    the internet's online pharmacy viagra - xenica...     1\n",
       "1    dear consumers, increase your business sales! ...     1\n",
       "2    1) fight the risk of cancer! http://www.adclic...     1\n",
       "3    we are offering you quality marketing lists wh...     1\n",
       "4    <html> <body> <font face=\"ms sans serif\"> <fon...     1\n",
       "..                                                 ...   ...\n",
       "911  >>>>> \"g\" == geege schuman <geege@barrera.org>...     0\n",
       "912  > i'm not sure what you mean by \"let's you and...     0\n",
       "913  >>>robert elz said: > date: wed, 28 aug 2002 0...     0\n",
       "914  chuck murcko wrote: > heh, ten years ago sayin...     0\n",
       "915  how about this: a bored forker said: \"yawn\" i ...     0\n",
       "\n",
       "[916 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data = pd.read_csv(\"result.csv\")[[\"Message\", \"Spam\"]]\n",
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.00527194794267416 0.5052719479426742 0\n",
      "1 0.0005734064616262913 0.5005734064616263 0\n",
      "1 0.018478073179721832 0.5184780731797218 0\n",
      "1 0.007929597981274128 0.5079295979812741 0\n",
      "1 0.0011583389714360237 0.501158338971436 0\n",
      "1 0.00954822264611721 0.5095482226461172 0\n",
      "1 0.0005734064616262913 0.5005734064616263 0\n",
      "1 0.24119049310684204 0.741190493106842 0\n",
      "0 0.9999834299087524 1.4999834299087524 1\n",
      "0 0.9840256571769714 1.4840256571769714 1\n",
      "0 0.999941349029541 1.499941349029541 1\n",
      "0 0.9999878406524658 1.4999878406524658 1\n",
      "0 0.9999843835830688 1.4999843835830688 1\n",
      "0 0.9999508857727051 1.499950885772705 1\n",
      "0 0.9937278032302856 1.4937278032302856 1\n",
      "0 0.9999626874923706 1.4999626874923706 1\n",
      "0 0.9999582767486572 1.4999582767486572 1\n",
      "0 0.9999899864196777 1.4999899864196777 1\n",
      "0 0.9999951124191284 1.4999951124191284 1\n",
      "0 0.9999264478683472 1.4999264478683472 1\n",
      "0 0.9999904632568359 1.499990463256836 1\n",
      "0 0.9999294281005859 1.499929428100586 1\n",
      "0 0.9999895095825195 1.4999895095825195 1\n",
      "0 0.9999674558639526 1.4999674558639526 1\n",
      "0 0.9999967813491821 1.4999967813491821 1\n",
      "0 0.9999622106552124 1.4999622106552124 1\n",
      "0 0.999972939491272 1.499972939491272 1\n",
      "0 0.9996122717857361 1.499612271785736 1\n",
      "0 0.9999608993530273 1.4999608993530273 1\n",
      "0 0.9993577599525452 1.4993577599525452 1\n",
      "0 0.9988731741905212 1.4988731741905212 1\n",
      "0 0.9999630451202393 1.4999630451202393 1\n",
      "0 0.9999582767486572 1.4999582767486572 1\n",
      "0 0.9995856881141663 1.4995856881141663 1\n",
      "0 0.9999551773071289 1.499955177307129 1\n",
      "0 0.9999747276306152 1.4999747276306152 1\n",
      "0 0.9995641112327576 1.4995641112327576 1\n",
      "0 0.9999908208847046 1.4999908208847046 1\n",
      "0 0.9999898672103882 1.4999898672103882 1\n",
      "0 0.9980831146240234 1.4980831146240234 1\n",
      "0 0.999983549118042 1.499983549118042 1\n",
      "0 0.946491003036499 1.446491003036499 1\n",
      "0 0.9476920366287231 1.4476920366287231 1\n",
      "0 0.9999804496765137 1.4999804496765137 1\n",
      "0 0.9999912977218628 1.4999912977218628 1\n",
      "0 0.9999951124191284 1.4999951124191284 1\n",
      "0 0.9989492297172546 1.4989492297172546 1\n",
      "0 0.9992107152938843 1.4992107152938843 1\n",
      "0 0.999206006526947 1.499206006526947 1\n",
      "0 0.9990109205245972 1.4990109205245972 1\n",
      "0 0.9999884366989136 1.4999884366989136 1\n",
      "0 0.9999582767486572 1.4999582767486572 1\n",
      "0 0.9999674558639526 1.4999674558639526 1\n",
      "0 0.999968409538269 1.499968409538269 1\n",
      "0 0.9997147917747498 1.4997147917747498 1\n",
      "0 0.9999808073043823 1.4999808073043823 1\n",
      "0 0.999803364276886 1.499803364276886 1\n",
      "0 0.9999804496765137 1.4999804496765137 1\n",
      "0 0.9999786615371704 1.4999786615371704 1\n",
      "0 0.9999961853027344 1.4999961853027344 1\n",
      "0 0.9999806880950928 1.4999806880950928 1\n",
      "0 0.9999674558639526 1.4999674558639526 1\n",
      "0 0.9999310970306396 1.4999310970306396 1\n",
      "0 0.9999879598617554 1.4999879598617554 1\n",
      "0 0.9996625185012817 1.4996625185012817 1\n",
      "0 0.9658681154251099 1.4658681154251099 1\n",
      "0 0.9996153116226196 1.4996153116226196 1\n",
      "0 0.9999873638153076 1.4999873638153076 1\n",
      "0 0.9999806880950928 1.4999806880950928 1\n",
      "0 0.9999551773071289 1.499955177307129 1\n",
      "0 0.9996827840805054 1.4996827840805054 1\n",
      "0 0.9990813732147217 1.4990813732147217 1\n",
      "0 0.9991718530654907 1.4991718530654907 1\n",
      "0 0.9999945163726807 1.4999945163726807 1\n",
      "0 0.999988317489624 1.499988317489624 1\n",
      "0 0.999755322933197 1.499755322933197 1\n",
      "0 0.9999932050704956 1.4999932050704956 1\n",
      "0 0.9999798536300659 1.499979853630066 1\n",
      "0 0.9995211362838745 1.4995211362838745 1\n",
      "0 0.9994025230407715 1.4994025230407715 1\n",
      "0 0.9999624490737915 1.4999624490737915 1\n",
      "0 0.9999852180480957 1.4999852180480957 1\n",
      "0 0.9999942779541016 1.4999942779541016 1\n",
      "0 0.9999696016311646 1.4999696016311646 1\n",
      "0 0.9996100068092346 1.4996100068092346 1\n",
      "0 0.9999374151229858 1.4999374151229858 1\n",
      "0 0.9997250437736511 1.4997250437736511 1\n",
      "0 0.9999632835388184 1.4999632835388184 1\n",
      "0 0.9947157502174377 1.4947157502174377 1\n",
      "0 0.9999924898147583 1.4999924898147583 1\n",
      "0 0.9995613694190979 1.499561369419098 1\n",
      "0 0.9999955892562866 1.4999955892562866 1\n",
      "0 0.9999685287475586 1.4999685287475586 1\n",
      "0 0.9999285936355591 1.499928593635559 1\n",
      "0 0.9999507665634155 1.4999507665634155 1\n",
      "0 0.999985933303833 1.499985933303833 1\n",
      "0 0.9999634027481079 1.499963402748108 1\n",
      "0 0.9999853372573853 1.4999853372573853 1\n",
      "0 0.9999860525131226 1.4999860525131226 1\n",
      "0 0.9998237490653992 1.4998237490653992 1\n",
      "0 0.9999681711196899 1.49996817111969 1\n",
      "0 0.9987504482269287 1.4987504482269287 1\n",
      "0 0.9998595714569092 1.4998595714569092 1\n",
      "0 0.9999892711639404 1.4999892711639404 1\n",
      "0 0.9990100860595703 1.4990100860595703 1\n",
      "0 0.9999146461486816 1.4999146461486816 1\n",
      "0 0.9999649524688721 1.499964952468872 1\n",
      "0 0.6684565544128418 1.1684565544128418 1\n",
      "0 0.9999527931213379 1.499952793121338 1\n",
      "0 0.9370263814926147 1.4370263814926147 1\n",
      "0 0.9999791383743286 1.4999791383743286 1\n",
      "0 0.9999936819076538 1.4999936819076538 1\n",
      "0 0.9999274015426636 1.4999274015426636 1\n",
      "0 0.9995463490486145 1.4995463490486145 1\n",
      "0 0.9999314546585083 1.4999314546585083 1\n",
      "0 0.9983183145523071 1.4983183145523071 1\n",
      "0 0.9999818801879883 1.4999818801879883 1\n",
      "0 0.9999810457229614 1.4999810457229614 1\n",
      "0 0.9998894929885864 1.4998894929885864 1\n",
      "0 0.9707141518592834 1.4707141518592834 1\n",
      "0 0.9999823570251465 1.4999823570251465 1\n",
      "0 0.999064028263092 1.499064028263092 1\n",
      "0 0.9999686479568481 1.4999686479568481 1\n",
      "0 0.9974504113197327 1.4974504113197327 1\n",
      "0 0.9999920129776001 1.4999920129776 1\n",
      "0 0.9999674558639526 1.4999674558639526 1\n",
      "0 0.9998313188552856 1.4998313188552856 1\n",
      "0 0.9999585151672363 1.4999585151672363 1\n",
      "0 0.9999881982803345 1.4999881982803345 1\n",
      "0 0.9999890327453613 1.4999890327453613 1\n",
      "0 0.9940817952156067 1.4940817952156067 1\n",
      "0 0.9999721050262451 1.4999721050262451 1\n",
      "0 0.9999828338623047 1.4999828338623047 1\n",
      "0 0.9995645880699158 1.4995645880699158 1\n",
      "0 0.999854326248169 1.499854326248169 1\n",
      "0 0.9994298815727234 1.4994298815727234 1\n",
      "0 0.9998692274093628 1.4998692274093628 1\n",
      "0 0.9999700784683228 1.4999700784683228 1\n",
      "0 0.9976708292961121 1.497670829296112 1\n",
      "0 0.9999889135360718 1.4999889135360718 1\n",
      "0 0.9996834993362427 1.4996834993362427 1\n",
      "0 0.999977707862854 1.499977707862854 1\n",
      "0 0.9998264908790588 1.4998264908790588 1\n",
      "0 0.9999905824661255 1.4999905824661255 1\n",
      "0 0.9993882179260254 1.4993882179260254 1\n",
      "0 0.9999678134918213 1.4999678134918213 1\n",
      "0 0.9998239874839783 1.4998239874839783 1\n",
      "0 0.9998270869255066 1.4998270869255066 1\n",
      "0 0.9997158646583557 1.4997158646583557 1\n",
      "0 0.9999876022338867 1.4999876022338867 1\n",
      "0 0.9996154308319092 1.4996154308319092 1\n",
      "0 0.9999806880950928 1.4999806880950928 1\n",
      "0 0.9999916553497314 1.4999916553497314 1\n",
      "0 0.9999697208404541 1.499969720840454 1\n",
      "0 0.9999943971633911 1.4999943971633911 1\n",
      "0 0.999984860420227 1.499984860420227 1\n",
      "0 0.999980092048645 1.499980092048645 1\n",
      "0 0.9998394250869751 1.499839425086975 1\n",
      "0 0.9999617338180542 1.4999617338180542 1\n",
      "0 0.8204799294471741 1.320479929447174 1\n",
      "0 0.9999791383743286 1.4999791383743286 1\n",
      "0 0.9999792575836182 1.4999792575836182 1\n",
      "0 0.9989160299301147 1.4989160299301147 1\n",
      "0 0.9994418025016785 1.4994418025016785 1\n",
      "0 0.9996962547302246 1.4996962547302246 1\n",
      "0 0.9999328851699829 1.499932885169983 1\n",
      "0 0.9996510744094849 1.4996510744094849 1\n",
      "0 0.9999924898147583 1.4999924898147583 1\n",
      "0 0.9992713332176208 1.4992713332176208 1\n",
      "0 0.9996585845947266 1.4996585845947266 1\n",
      "0 0.998783528804779 1.498783528804779 1\n",
      "0 0.9999536275863647 1.4999536275863647 1\n",
      "0 0.9996591806411743 1.4996591806411743 1\n",
      "0 0.9296093583106995 1.4296093583106995 1\n",
      "0 0.99980229139328 1.49980229139328 1\n",
      "0 0.9999353885650635 1.4999353885650635 1\n",
      "0 0.999757707118988 1.499757707118988 1\n",
      "0 0.9952946305274963 1.4952946305274963 1\n",
      "0 0.972614586353302 1.472614586353302 1\n",
      "0 0.9995782971382141 1.4995782971382141 1\n",
      "0 0.9994487166404724 1.4994487166404724 1\n",
      "0 0.9999500513076782 1.4999500513076782 1\n",
      "0 0.9999678134918213 1.4999678134918213 1\n",
      "0 0.9997856020927429 1.499785602092743 1\n",
      "0 0.9998699426651001 1.4998699426651 1\n",
      "0 0.9999979734420776 1.4999979734420776 1\n",
      "0 0.9999468326568604 1.4999468326568604 1\n",
      "0 0.999910831451416 1.499910831451416 1\n",
      "0 0.9998650550842285 1.4998650550842285 1\n",
      "0 0.9999085664749146 1.4999085664749146 1\n",
      "0 0.999870777130127 1.499870777130127 1\n",
      "0 0.9999802112579346 1.4999802112579346 1\n",
      "0 0.9988435506820679 1.4988435506820679 1\n",
      "0 0.9998701810836792 1.4998701810836792 1\n",
      "0 0.9999699592590332 1.4999699592590332 1\n",
      "0 0.9998482465744019 1.4998482465744019 1\n",
      "0 0.8505263924598694 1.3505263924598694 1\n",
      "0 0.9999929666519165 1.4999929666519165 1\n",
      "0 0.9999924898147583 1.4999924898147583 1\n",
      "0 0.9925764203071594 1.4925764203071594 1\n",
      "0 0.9999852180480957 1.4999852180480957 1\n",
      "0 0.9998500347137451 1.4998500347137451 1\n",
      "0 0.9999850988388062 1.4999850988388062 1\n",
      "0 0.9999958276748657 1.4999958276748657 1\n",
      "0 0.9998817443847656 1.4998817443847656 1\n",
      "0 0.9999862909317017 1.4999862909317017 1\n",
      "0 0.9998388290405273 1.4998388290405273 1\n",
      "0 0.9999198913574219 1.4999198913574219 1\n",
      "0 0.9999231100082397 1.4999231100082397 1\n",
      "0 0.9999821186065674 1.4999821186065674 1\n",
      "0 0.9999648332595825 1.4999648332595825 1\n",
      "0 0.9999102354049683 1.4999102354049683 1\n",
      "0 0.9999692440032959 1.499969244003296 1\n",
      "0 0.9999480247497559 1.4999480247497559 1\n",
      "0 0.9999639987945557 1.4999639987945557 1\n",
      "0 0.9999592304229736 1.4999592304229736 1\n",
      "0 0.9998999834060669 1.499899983406067 1\n",
      "0 0.9963905215263367 1.4963905215263367 1\n",
      "0 0.9998511075973511 1.499851107597351 1\n",
      "0 0.9999648332595825 1.4999648332595825 1\n",
      "0 0.9998316764831543 1.4998316764831543 1\n",
      "0 0.9974966645240784 1.4974966645240784 1\n",
      "0 0.9998966455459595 1.4998966455459595 1\n",
      "0 0.9999699592590332 1.4999699592590332 1\n",
      "0 0.9999858140945435 1.4999858140945435 1\n",
      "0 0.9999581575393677 1.4999581575393677 1\n",
      "0 0.721388578414917 1.221388578414917 1\n",
      "0 0.9999904632568359 1.499990463256836 1\n",
      "0 0.9959831237792969 1.4959831237792969 1\n",
      "0 0.9999548196792603 1.4999548196792603 1\n",
      "0 0.9998595714569092 1.4998595714569092 1\n",
      "0 0.9999549388885498 1.4999549388885498 1\n",
      "0 0.9999679327011108 1.4999679327011108 1\n",
      "0 0.9983183145523071 1.4983183145523071 1\n",
      "0 0.9998559951782227 1.4998559951782227 1\n",
      "0 0.9999852180480957 1.4999852180480957 1\n",
      "0 0.9996639490127563 1.4996639490127563 1\n",
      "0 0.9992470741271973 1.4992470741271973 1\n",
      "0 0.9986923336982727 1.4986923336982727 1\n",
      "0 0.999985933303833 1.499985933303833 1\n",
      "0 0.9999637603759766 1.4999637603759766 1\n",
      "0 0.9989410042762756 1.4989410042762756 1\n",
      "0 0.9993873834609985 1.4993873834609985 1\n",
      "0 0.6207432746887207 1.1207432746887207 1\n",
      "0 0.9999812841415405 1.4999812841415405 1\n",
      "0 0.9998204112052917 1.4998204112052917 1\n",
      "0 0.999624490737915 1.499624490737915 1\n",
      "0 0.913815438747406 1.413815438747406 1\n",
      "0 0.6214519143104553 1.1214519143104553 1\n",
      "0 0.9998290538787842 1.4998290538787842 1\n",
      "0 0.9998397827148438 1.4998397827148438 1\n",
      "0 0.9996627569198608 1.4996627569198608 1\n",
      "0 0.9999971389770508 1.4999971389770508 1\n",
      "0 0.9999024868011475 1.4999024868011475 1\n",
      "0 0.9999535083770752 1.4999535083770752 1\n",
      "0 0.9997060894966125 1.4997060894966125 1\n",
      "0 0.9993358254432678 1.4993358254432678 1\n",
      "0 0.9996417760848999 1.4996417760849 1\n",
      "0 0.9999812841415405 1.4999812841415405 1\n",
      "0 0.9990270137786865 1.4990270137786865 1\n",
      "0 0.9998983144760132 1.4998983144760132 1\n",
      "0 0.9999492168426514 1.4999492168426514 1\n",
      "0 0.9999672174453735 1.4999672174453735 1\n",
      "0 0.9999648332595825 1.4999648332595825 1\n",
      "0 0.8264396786689758 1.3264396786689758 1\n",
      "0 0.9999922513961792 1.4999922513961792 1\n",
      "0 0.9998202919960022 1.4998202919960022 1\n",
      "0 0.9999494552612305 1.4999494552612305 1\n",
      "0 0.9999943971633911 1.4999943971633911 1\n",
      "0 0.9999867677688599 1.4999867677688599 1\n",
      "0 0.9999915361404419 1.499991536140442 1\n",
      "0 0.9999915361404419 1.499991536140442 1\n",
      "0 0.8656591773033142 1.3656591773033142 1\n",
      "0 0.9999934434890747 1.4999934434890747 1\n",
      "0 0.9833828806877136 1.4833828806877136 1\n",
      "0 0.9830098748207092 1.4830098748207092 1\n",
      "0 0.9992799162864685 1.4992799162864685 1\n",
      "0 0.9999735355377197 1.4999735355377197 1\n",
      "0 0.9999511241912842 1.4999511241912842 1\n",
      "0 0.9999083280563354 1.4999083280563354 1\n",
      "0 0.9999592304229736 1.4999592304229736 1\n",
      "0 0.9989472031593323 1.4989472031593323 1\n",
      "0 0.9997850060462952 1.4997850060462952 1\n",
      "0 0.9999281167984009 1.4999281167984009 1\n",
      "0 0.999970555305481 1.499970555305481 1\n",
      "0 0.9873851537704468 1.4873851537704468 1\n",
      "0 0.9999971389770508 1.4999971389770508 1\n",
      "0 0.9999818801879883 1.4999818801879883 1\n",
      "0 0.9996262788772583 1.4996262788772583 1\n",
      "0 0.9999845027923584 1.4999845027923584 1\n",
      "0 0.9999823570251465 1.4999823570251465 1\n",
      "0 0.9828100204467773 1.4828100204467773 1\n",
      "0 0.9999881982803345 1.4999881982803345 1\n",
      "0 0.9999706745147705 1.4999706745147705 1\n",
      "0 0.7117433547973633 1.2117433547973633 1\n",
      "0 0.9998582601547241 1.4998582601547241 1\n",
      "0 0.9996563196182251 1.499656319618225 1\n",
      "0 0.9998754262924194 1.4998754262924194 1\n",
      "0 0.9999885559082031 1.4999885559082031 1\n",
      "0 0.9999970197677612 1.4999970197677612 1\n",
      "0 0.9999909400939941 1.4999909400939941 1\n",
      "0 0.9960200190544128 1.4960200190544128 1\n",
      "0 0.9996305704116821 1.4996305704116821 1\n",
      "0 0.9975288510322571 1.497528851032257 1\n",
      "0 0.9982511401176453 1.4982511401176453 1\n",
      "0 0.9998750686645508 1.4998750686645508 1\n",
      "0 0.9999880790710449 1.499988079071045 1\n",
      "0 0.9999957084655762 1.4999957084655762 1\n",
      "0 0.9994087219238281 1.4994087219238281 1\n",
      "0 0.9993258714675903 1.4993258714675903 1\n",
      "0 0.9998005032539368 1.4998005032539368 1\n",
      "0 0.9999819993972778 1.4999819993972778 1\n",
      "0 0.999790370464325 1.499790370464325 1\n",
      "0 0.9999783039093018 1.4999783039093018 1\n",
      "0 0.8587046265602112 1.3587046265602112 1\n",
      "0 0.9940734505653381 1.4940734505653381 1\n",
      "0 0.9980453252792358 1.4980453252792358 1\n",
      "0 0.9999359846115112 1.4999359846115112 1\n",
      "0 0.9990134239196777 1.4990134239196777 1\n",
      "0 0.9134575128555298 1.4134575128555298 1\n",
      "0 0.9865626692771912 1.4865626692771912 1\n",
      "0 0.9988576173782349 1.4988576173782349 1\n",
      "0 0.9999082088470459 1.499908208847046 1\n",
      "0 0.9999945163726807 1.4999945163726807 1\n",
      "0 0.999910831451416 1.499910831451416 1\n",
      "0 0.9405418634414673 1.4405418634414673 1\n",
      "0 0.9998705387115479 1.4998705387115479 1\n",
      "0 0.9999878406524658 1.4999878406524658 1\n",
      "0 0.9995071887969971 1.499507188796997 1\n",
      "0 0.999826967716217 1.499826967716217 1\n",
      "0 0.9998492002487183 1.4998492002487183 1\n",
      "0 0.9998576641082764 1.4998576641082764 1\n",
      "0 0.999757707118988 1.499757707118988 1\n",
      "0 0.9999585151672363 1.4999585151672363 1\n",
      "0 0.9999535083770752 1.4999535083770752 1\n",
      "0 0.9998264908790588 1.4998264908790588 1\n",
      "0 0.9982328414916992 1.4982328414916992 1\n",
      "0 0.9997498393058777 1.4997498393058777 1\n",
      "0 0.9979262351989746 1.4979262351989746 1\n",
      "0 0.9997058510780334 1.4997058510780334 1\n",
      "0 0.9991922974586487 1.4991922974586487 1\n",
      "0 0.9999805688858032 1.4999805688858032 1\n",
      "0 0.9999526739120483 1.4999526739120483 1\n",
      "0 0.9997151494026184 1.4997151494026184 1\n",
      "0 0.9998422861099243 1.4998422861099243 1\n",
      "0 0.9999632835388184 1.4999632835388184 1\n",
      "0 0.999980092048645 1.499980092048645 1\n",
      "0 0.9999783039093018 1.4999783039093018 1\n",
      "0 0.9999939203262329 1.499993920326233 1\n",
      "0 0.9999455213546753 1.4999455213546753 1\n",
      "0 0.9997937083244324 1.4997937083244324 1\n",
      "0 0.9979402422904968 1.4979402422904968 1\n",
      "0 0.9998989105224609 1.499898910522461 1\n",
      "0 0.9995699524879456 1.4995699524879456 1\n",
      "0 0.9982328414916992 1.4982328414916992 1\n",
      "0 0.9999520778656006 1.4999520778656006 1\n",
      "0 0.9999606609344482 1.4999606609344482 1\n",
      "0 0.9998958110809326 1.4998958110809326 1\n",
      "0 0.9999489784240723 1.4999489784240723 1\n",
      "0 0.9999547004699707 1.4999547004699707 1\n",
      "0 0.9995012283325195 1.4995012283325195 1\n",
      "0 0.9999674558639526 1.4999674558639526 1\n",
      "0 0.9999834299087524 1.4999834299087524 1\n",
      "0 0.999995231628418 1.499995231628418 1\n",
      "0 0.9999905824661255 1.4999905824661255 1\n",
      "0 0.9999544620513916 1.4999544620513916 1\n",
      "0 0.9999940395355225 1.4999940395355225 1\n",
      "0 0.9999688863754272 1.4999688863754272 1\n",
      "0 0.9999902248382568 1.4999902248382568 1\n",
      "0 0.9999701976776123 1.4999701976776123 1\n",
      "0 0.9999768733978271 1.4999768733978271 1\n",
      "0 0.9999392032623291 1.499939203262329 1\n",
      "0 0.999993085861206 1.499993085861206 1\n",
      "0 0.9999345541000366 1.4999345541000366 1\n",
      "0 0.9999421834945679 1.4999421834945679 1\n",
      "0 0.9999860525131226 1.4999860525131226 1\n",
      "0 0.9999897480010986 1.4999897480010986 1\n",
      "0 0.9998507499694824 1.4998507499694824 1\n",
      "0 0.9996557235717773 1.4996557235717773 1\n",
      "0 0.99977046251297 1.49977046251297 1\n",
      "0 0.9999725818634033 1.4999725818634033 1\n",
      "0 0.99998939037323 1.49998939037323 1\n",
      "0 0.9999905824661255 1.4999905824661255 1\n",
      "0 0.9999338388442993 1.4999338388442993 1\n",
      "0 0.9999185800552368 1.4999185800552368 1\n",
      "0 0.9999504089355469 1.4999504089355469 1\n",
      "0 0.999798595905304 1.499798595905304 1\n",
      "0 0.9978190660476685 1.4978190660476685 1\n",
      "0 0.9999188184738159 1.499918818473816 1\n",
      "0 0.9993669390678406 1.4993669390678406 1\n",
      "0 0.9999760389328003 1.4999760389328003 1\n",
      "0 0.999786913394928 1.499786913394928 1\n",
      "0 0.9999853372573853 1.4999853372573853 1\n",
      "0 0.9999908208847046 1.4999908208847046 1\n",
      "0 0.9999573230743408 1.4999573230743408 1\n",
      "0 0.9999921321868896 1.4999921321868896 1\n",
      "0 0.9999635219573975 1.4999635219573975 1\n",
      "0 0.9999809265136719 1.4999809265136719 1\n",
      "0 0.9998239874839783 1.4998239874839783 1\n",
      "0 0.9992138147354126 1.4992138147354126 1\n",
      "0 0.9999855756759644 1.4999855756759644 1\n",
      "0 0.9999735355377197 1.4999735355377197 1\n",
      "0 0.9999849796295166 1.4999849796295166 1\n",
      "0 0.9999890327453613 1.4999890327453613 1\n",
      "0 0.9996585845947266 1.4996585845947266 1\n",
      "0 0.9999768733978271 1.4999768733978271 1\n",
      "0 0.9923370480537415 1.4923370480537415 1\n",
      "0 0.9999340772628784 1.4999340772628784 1\n",
      "0 0.9998791217803955 1.4998791217803955 1\n",
      "0 0.9987291693687439 1.498729169368744 1\n",
      "0 0.9990745782852173 1.4990745782852173 1\n",
      "0 0.9956602454185486 1.4956602454185486 1\n",
      "0 0.9999067783355713 1.4999067783355713 1\n",
      "0 0.9996248483657837 1.4996248483657837 1\n",
      "0 0.9999833106994629 1.499983310699463 1\n",
      "0 0.9999724626541138 1.4999724626541138 1\n",
      "0 0.9999710321426392 1.4999710321426392 1\n",
      "0 0.9931440353393555 1.4931440353393555 1\n",
      "0 0.9998273253440857 1.4998273253440857 1\n",
      "0 0.9990848302841187 1.4990848302841187 1\n",
      "0 0.9989927411079407 1.4989927411079407 1\n",
      "0 0.9999773502349854 1.4999773502349854 1\n",
      "0 0.8904658555984497 1.3904658555984497 1\n",
      "0 0.999576985836029 1.499576985836029 1\n",
      "0 0.9999560117721558 1.4999560117721558 1\n",
      "0 0.9999644756317139 1.4999644756317139 1\n",
      "0 0.9618782997131348 1.4618782997131348 1\n",
      "0 0.9999803304672241 1.4999803304672241 1\n",
      "0 0.9998974800109863 1.4998974800109863 1\n",
      "0 0.9999136924743652 1.4999136924743652 1\n",
      "0 0.9997712969779968 1.4997712969779968 1\n",
      "0 0.9993390440940857 1.4993390440940857 1\n",
      "0 0.9995978474617004 1.4995978474617004 1\n",
      "0 0.8814899921417236 1.3814899921417236 1\n",
      "0 0.9996649026870728 1.4996649026870728 1\n",
      "0 0.9998607635498047 1.4998607635498047 1\n",
      "0 0.9999901056289673 1.4999901056289673 1\n",
      "0 0.9999402761459351 1.499940276145935 1\n",
      "0 0.9997852444648743 1.4997852444648743 1\n",
      "0 0.9999340772628784 1.4999340772628784 1\n",
      "0 0.9999765157699585 1.4999765157699585 1\n",
      "0 0.999923825263977 1.499923825263977 1\n",
      "0 0.9999784231185913 1.4999784231185913 1\n",
      "0 0.940048336982727 1.440048336982727 1\n",
      "0 0.9999347925186157 1.4999347925186157 1\n",
      "0 0.9997976422309875 1.4997976422309875 1\n",
      "0 0.9998095631599426 1.4998095631599426 1\n",
      "0 0.9998297691345215 1.4998297691345215 1\n",
      "0 0.999962568283081 1.499962568283081 1\n",
      "0 0.9998651742935181 1.499865174293518 1\n",
      "0 0.8219136595726013 1.3219136595726013 1\n",
      "0 0.9997921586036682 1.4997921586036682 1\n",
      "0 0.9849721789360046 1.4849721789360046 1\n",
      "0 0.9968294501304626 1.4968294501304626 1\n",
      "0 0.9602143168449402 1.4602143168449402 1\n",
      "0 0.9999551773071289 1.499955177307129 1\n",
      "0 0.999985933303833 1.499985933303833 1\n",
      "0 0.9867051243782043 1.4867051243782043 1\n",
      "0 0.9999874830245972 1.4999874830245972 1\n",
      "0 0.9999809265136719 1.4999809265136719 1\n",
      "0 0.9999512434005737 1.4999512434005737 1\n",
      "0 0.9998767375946045 1.4998767375946045 1\n",
      "0 0.9991668462753296 1.4991668462753296 1\n",
      "0 0.9998272061347961 1.4998272061347961 1\n",
      "0 0.9998537302017212 1.4998537302017212 1\n",
      "0 0.9996834993362427 1.4996834993362427 1\n",
      "0 0.9999624490737915 1.4999624490737915 1\n",
      "0 0.9999285936355591 1.499928593635559 1\n",
      "0 0.999985933303833 1.499985933303833 1\n",
      "0 0.9999254941940308 1.4999254941940308 1\n",
      "0 0.9999817609786987 1.4999817609786987 1\n",
      "0 0.9997019171714783 1.4997019171714783 1\n",
      "0 0.9999933242797852 1.4999933242797852 1\n",
      "0 0.999525785446167 1.499525785446167 1\n",
      "0 0.9999842643737793 1.4999842643737793 1\n",
      "0 0.9999585151672363 1.4999585151672363 1\n",
      "0 0.9994275569915771 1.4994275569915771 1\n",
      "0 0.9937723278999329 1.4937723278999329 1\n",
      "0 0.9999911785125732 1.4999911785125732 1\n",
      "0 0.9217915534973145 1.4217915534973145 1\n",
      "0 0.9997840523719788 1.4997840523719788 1\n",
      "0 0.9999943971633911 1.4999943971633911 1\n",
      "0 0.9999895095825195 1.4999895095825195 1\n",
      "0 0.9997382760047913 1.4997382760047913 1\n",
      "0 0.9999788999557495 1.4999788999557495 1\n",
      "0 0.9997480511665344 1.4997480511665344 1\n",
      "0 0.999963641166687 1.499963641166687 1\n",
      "0 0.9999675750732422 1.4999675750732422 1\n",
      "0 0.9999890327453613 1.4999890327453613 1\n",
      "0 0.997724711894989 1.497724711894989 1\n",
      "0 0.9981644749641418 1.4981644749641418 1\n",
      "0 0.9994744658470154 1.4994744658470154 1\n",
      "0 0.9999722242355347 1.4999722242355347 1\n",
      "0 0.9994083642959595 1.4994083642959595 1\n",
      "0 0.9984623193740845 1.4984623193740845 1\n",
      "0 0.999935507774353 1.499935507774353 1\n",
      "0 0.9990695118904114 1.4990695118904114 1\n",
      "0 0.997288703918457 1.497288703918457 1\n",
      "0 0.9999854564666748 1.4999854564666748 1\n",
      "0 0.9999548196792603 1.4999548196792603 1\n",
      "0 0.9999250173568726 1.4999250173568726 1\n",
      "0 0.9999783039093018 1.4999783039093018 1\n",
      "0 0.9999663829803467 1.4999663829803467 1\n",
      "0 0.9088250398635864 1.4088250398635864 1\n",
      "0 0.9999274015426636 1.4999274015426636 1\n",
      "0 0.9989858269691467 1.4989858269691467 1\n",
      "0 0.9999608993530273 1.4999608993530273 1\n",
      "0 0.9999722242355347 1.4999722242355347 1\n",
      "0 0.9999945163726807 1.4999945163726807 1\n",
      "0 0.9999524354934692 1.4999524354934692 1\n",
      "0 0.9999892711639404 1.4999892711639404 1\n",
      "0 0.9979717135429382 1.4979717135429382 1\n",
      "0 0.9999428987503052 1.4999428987503052 1\n",
      "0 0.9999401569366455 1.4999401569366455 1\n",
      "0 0.9999843835830688 1.4999843835830688 1\n",
      "0 0.9998016953468323 1.4998016953468323 1\n",
      "0 0.9997772574424744 1.4997772574424744 1\n",
      "0 0.999951958656311 1.499951958656311 1\n",
      "0 0.9975703358650208 1.4975703358650208 1\n",
      "0 0.9999427795410156 1.4999427795410156 1\n",
      "0 0.9994381070137024 1.4994381070137024 1\n",
      "0 0.9999707937240601 1.49997079372406 1\n",
      "0 0.5783934593200684 1.0783934593200684 1\n",
      "0 0.9994840621948242 1.4994840621948242 1\n",
      "0 0.9997884631156921 1.4997884631156921 1\n",
      "0 0.9657989144325256 1.4657989144325256 1\n",
      "0 0.9999498128890991 1.4999498128890991 1\n",
      "0 0.9999394416809082 1.4999394416809082 1\n",
      "0 0.9999570846557617 1.4999570846557617 1\n",
      "0 0.999993085861206 1.499993085861206 1\n",
      "0 0.9998732805252075 1.4998732805252075 1\n",
      "0 0.9999388456344604 1.4999388456344604 1\n",
      "0 0.9978094696998596 1.4978094696998596 1\n",
      "0 0.9927543997764587 1.4927543997764587 1\n",
      "41.70305676855895\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "counter = 0\n",
    "valid = 0\n",
    "\n",
    "for index, row in valid_data.iterrows():\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', row[\"Message\"].strip())\n",
    "    if text:\n",
    "      #text = row[\"Message\"]\n",
    "      spam = row[\"Spam\"]\n",
    "      doc = nlp(text)\n",
    "      temp_spam = int(doc.cats[\"SPAM\"] + 0.5)\n",
    "      if temp_spam == spam:\n",
    "        valid += 1\n",
    "      counter += 1\n",
    "\n",
    "valid_acc = valid/counter * 100\n",
    "print(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
